





























from fastai.vision.all import *
from fastai.text.all import *
from datasets import Dataset
from captcha.image import ImageCaptcha
import random
import string


import string
from fastai.vision.all import *
from fastai.text.all import *
from captcha.image import ImageCaptcha

# 1. Create vocabulary from all possible characters (since we know possible chars)
chars = sorted(string.ascii_uppercase + string.digits)






# 2. Create a fake CAPTCHA dataset
def create_captcha_dataset(size=100):
    generator = ImageCaptcha(width=160, height=60)
    data = []
    for _ in range(size):
        label = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))
        img = generator.generate_image(label)
        data.append({'image': img, 'label': label})  # Using our custom class
    return Dataset.from_list(data)


def text_addition_rule(t, add_bos=True, add_eos=True):
    return (f'{BOS} ' if add_bos else '') + t + (f' {EOS}' if add_eos else '')

class CharTokenizerTransform(Transform):
    def __init__(self, special_tokens=None, sep='', rules=text_addition_rule):
        self.special_tokens = special_tokens or defaults.text_spec_tok
        self.sep = sep
        self.rules = L(ifnone(rules, defaults.text_proc_rules))

    def _tokenize1(self, o: str):
        # Apply preprocessing rules
        txt = compose(*self.rules)(o)
        tokens = []
        for word in txt.split(' '):
            if word in self.special_tokens:
                tokens.append(word)
            else:
                tokens.extend(list(word))
        return tokens

    def encodes(self, o: str):
        return self._tokenize1(o)

    def decodes(self, o):
        # Remove special tokens during decoding for display
        clean = [t for t in o if t not in self.special_tokens]
        return TitledStr(self.sep.join(clean))



vocab = make_vocab(Counter(chars), min_freq=1)





captcha_block = DataBlock(
    blocks=(ImageBlock, TextBlock(
        tok_tfm=CharTokenizerTransform,
        vocab=vocab
        
    )),
    get_x=lambda o: o['image'],
    get_y=lambda o: o['label'],
    splitter=RandomSplitter(0.2),
    item_tfms=None,
    batch_tfms=Normalize.from_stats(*imagenet_stats)
)



# 3. Generate the dataset
ds = create_captcha_dataset(1000)

dls = captcha_block.dataloaders(ds, bs=16)


x, y = dls.one_batch()


y


dls.show_batch()


show_image(ds[0]['image'],title=ds[0]['label'],figsize=(3,5))





class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(1))  # Shape: [max_len, 1, d_model]

    def forward(self, x: Tensor) -> Tensor:
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)
    
class CNNEncoder(nn.Module):
    def __init__(self, d_model=128):
        super().__init__()
        self.backbone = create_body(resnet18(), pretrained=True, cut=-2)
        self.conv = nn.Conv2d(512, d_model, 1)

    def forward(self, x):
        x = self.backbone(x)  # B x 512 x H x W
        x = self.conv(x)      # B x d_model x H x W
        B, C, H, W = x.shape
        return x.view(B, C, -1).permute(2, 0, 1)  # [Seq, B, d_model]
        
class TransformerDecoder(nn.Module):
    def __init__(self, vocab_size, d_model=128, nhead=8, num_layers=3, dim_feedforward=512, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout)
        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)
        self.fc = nn.Linear(d_model, vocab_size)
        self.d_model = d_model

    def forward(self, tgt, memory):
        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)
        tgt_emb = self.pos_encoder(tgt_emb)
        output = self.decoder(tgt_emb, memory)
        return self.fc(output)
    
class EncoderDecoder(nn.Module):
    def __init__(self, encoder, decoder, max_seq_len=20, sos_idx=2, eos_idx=3):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.max_seq_len = max_seq_len
        self.sos_idx = sos_idx
        self.eos_idx = eos_idx

    def forward(self, x, y=None):
        memory = self.encoder(x)  # [seq_len, B, d_model]

        # Teacher-forcing mode
        if y is not None:
            tgt = y[:, :-1].permute(1, 0)  # [T, B]
            pred = self.decoder(tgt, memory)
            return pred

        # Auto-regressive inference
        bs = x.size(0)
        tgt = torch.full((1, bs), self.sos_idx, dtype=torch.long, device=x.device)
        outputs = []

        for _ in range(self.max_seq_len - 1):
            logits = self.decoder(tgt, memory)  # [T, B, vocab]
            next_token = logits[-1].argmax(dim=-1).unsqueeze(0)  # [1, B]
            tgt = torch.cat([tgt, next_token], dim=0)
            outputs.append(next_token)

            if (next_token == self.eos_idx).all():
                break

        return tgt.permute(1, 0)  # [B, T]








from torch.nn.utils.rnn import pad_sequence

class SeqLoss(BaseLoss):
    def __init__(self, pad_idx, vocab, axis=-1):
        self.pad_idx = pad_idx
        self.vocab = vocab
        self.special_tokens = {'xxpad', 'xxbos', 'xxeos'}
        self.axis = axis
        super().__init__(nn.CrossEntropyLoss, axis=axis, ignore_index=pad_idx, reduction='mean')

    def __call__(self, output, target, **kwargs):
        # output: [T, B, vocab], target: [B, T]

        target = target[:, 1:]      # Skip <bos>
        return self.func(output.reshape(-1, output.shape[-1]), target.T.reshape(-1))

    def decodes(self, output):
        # output: [T, B, vocab]
        pred_tokens = output.argmax(dim=self.axis).permute(1, 0)  # [B, T]
        decoded = []
        for tokens in pred_tokens:
            data = [i.item() for i in tokens if self.vocab[i] not in self.special_tokens]
            decoded.append(torch.IntTensor(data))

        padded_tensor = pad_sequence(decoded, batch_first=True, padding_value=pad_idx)
        return TensorText(padded_tensor)
        
    def activation(self, output):
        return F.softmax(output, dim=self.axis)


from fastai.callback.core import Callback

class AddTargetToInput(Callback):
    def before_batch(self):
        # Combine input and target into a tuple
        self.learn.xb = (self.learn.xb[0], self.learn.yb[0])


pad_idx = vocab.index('xxpad')
bos_idx = vocab.index('xxbos')
eos_idx = vocab.index('xxeos')


vocab_size = len(vocab)
encoder = CNNEncoder(d_model=128)
decoder = TransformerDecoder(vocab_size, d_model=128)
model = EncoderDecoder(encoder, decoder, max_seq_len = 7, sos_idx = bos_idx, eos_idx=  eos_idx)
loss_func = SeqLoss(pad_idx=pad_idx, vocab=vocab)


from Levenshtein import distance as levenshtein_distance

def cer(preds, targets):
    """
    Compute the Character Error Rate (CER) between two lists of strings.
    """
    total_edits = 0
    total_chars = 0
    for pred, target in zip(preds, targets):
        total_edits += levenshtein_distance(pred, target)
        total_chars += len(target)
    return total_edits / total_chars if total_chars > 0 else 0


from fastai.metrics import AccumMetric

class CERMetric(AccumMetric):
    def __init__(self, vocab, **kwargs):
        self.vocab = vocab
        super().__init__(cer, **kwargs)

    def accumulate(self, learn):
        preds = learn.pred
        targs = learn.yb[0]

        # Decode predictions
        pred_tokens = preds.argmax(dim=-1).permute(1, 0)  # [B, T]
        pred_strs = [''.join([self.vocab[i] for i in seq if self.vocab[i] not in ['xxpad', 'xxbos', 'xxeos']]) for seq in pred_tokens]

        # Decode targets
        targ_strs = [''.join([self.vocab[i] for i in seq if self.vocab[i] not in ['xxpad', 'xxbos', 'xxeos']]) for seq in targs]

        self.preds += pred_strs
        self.targs += targ_strs
    


learn = Learner(
        dls,
        model,
        loss_func=loss_func,
        cbs=[AddTargetToInput()],
        wd=1e-3  # Reduced weight decay
)


learn.lr_find()


learn.fit_one_cycle(10, 1e-3)


from plum import dispatch
class CaptchaStr(str):
    def show(self, ctx=None, **kwargs):
        return show_title(self, ctx=ctx)


@dispatch
def show_results(x:TensorImage, y:TensorText, samples, outs, ctxs=None, max_n=6, **kwargs):
    
    # ctxs = get_grid(min(len(samples), max_n)) if not ctxs else ctxs
    if ctxs is None: ctxs = get_grid(min(len(samples), max_n))

    for i, ((img, tgt), pred) in enumerate(zip(samples, outs)):
        ctx = ctxs[i]
        img.show(ctx=ctx)

        # Decode target and prediction from TensorText
        tgt_str = CaptchaStr(tgt)
        pred_str = CaptchaStr(pred[0])
        
        ctx.set_title(f"Actual: {tgt_str}\nPred: {pred_str}", fontsize=8)
    
    return ctxs



learn.show_results()












